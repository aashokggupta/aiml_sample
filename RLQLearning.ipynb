{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5aKpMSW6XINHhTY22/d2P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "0EcCQvTrQ0Sv",
        "outputId": "e6983a2b-86dd-4605-e3eb-14cce78127c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4076596325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mcell_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q-learning Gridworld\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2347\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_debug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_print_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ],
      "source": [
        "import tkinter as tk\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "# -----------------------------\n",
        "# Environment setup (Gridworld)\n",
        "# -----------------------------\n",
        "\n",
        "grid_size = 5   # 5x5 grid -> total 25 states\n",
        "n_states = grid_size * grid_size\n",
        "actions = [0, 1, 2, 3]   # 0 = up, 1 = down, 2 = left, 3 = right\n",
        "terminal_state = n_states - 1  # bottom-right cell is goal\n",
        "\n",
        "# Rewards: reaching goal = +10, otherwise -1\n",
        "rewards = np.full(n_states, -1)\n",
        "rewards[terminal_state] = 10\n",
        "\n",
        "# -----------------------------\n",
        "# Q-learning setup\n",
        "# -----------------------------\n",
        "\n",
        "Q = np.zeros((n_states, len(actions)))   # Q-table initialized to zeros\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1      # learning rate\n",
        "gamma = 0.9      # discount factor\n",
        "epsilon = 0.6    # exploration rate\n",
        "episodes = 50    # number of training episodes\n",
        "\n",
        "# -----------------------------\n",
        "# Helper functions\n",
        "# -----------------------------\n",
        "\n",
        "def state_to_coords(state):\n",
        "    \"\"\"Convert state index into grid coordinates (row, col).\"\"\"\n",
        "    return divmod(state, grid_size)\n",
        "\n",
        "def coords_to_state(row, col):\n",
        "    \"\"\"Convert grid coordinates into state index.\"\"\"\n",
        "    return row * grid_size + col\n",
        "\n",
        "def step_env(state, action):\n",
        "    \"\"\"Take an action in the gridworld and return (next_state, reward).\"\"\"\n",
        "    row, col = state_to_coords(state)\n",
        "\n",
        "    if action == 0 and row > 0:              # up\n",
        "        row -= 1\n",
        "    elif action == 1 and row < grid_size - 1:  # down\n",
        "        row += 1\n",
        "    elif action == 2 and col > 0:            # left\n",
        "        col -= 1\n",
        "    elif action == 3 and col < grid_size - 1:  # right\n",
        "        col += 1\n",
        "\n",
        "    next_state = coords_to_state(row, col)\n",
        "    return next_state, rewards[next_state]\n",
        "\n",
        "def choose_action(state):\n",
        "    \"\"\"Epsilon-greedy policy with random tie-breaking.\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)  # explore\n",
        "    qvals = Q[state]\n",
        "    max_q = np.max(qvals)\n",
        "    best_actions = np.where(qvals == max_q)[0]\n",
        "    return np.random.choice(best_actions)  # exploit\n",
        "\n",
        "# -----------------------------\n",
        "# GUI setup\n",
        "# -----------------------------\n",
        "\n",
        "cell_size = 80\n",
        "root = tk.Tk()\n",
        "root.title(\"Q-learning Gridworld\")\n",
        "\n",
        "canvas = tk.Canvas(root, width=grid_size * cell_size, height=grid_size * cell_size)\n",
        "canvas.pack()\n",
        "\n",
        "# Draw the grid cells\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        x1, y1 = j * cell_size, i * cell_size\n",
        "        x2, y2 = x1 + cell_size, y1 + cell_size\n",
        "        color = \"white\"\n",
        "        if coords_to_state(i, j) == terminal_state:\n",
        "            color = \"lightgreen\"  # goal cell\n",
        "        canvas.create_rectangle(x1, y1, x2, y2, fill=color, outline=\"black\")\n",
        "\n",
        "# Agent (red circle)\n",
        "agent = canvas.create_oval(5, 5, cell_size - 5, cell_size - 5, fill=\"red\")\n",
        "\n",
        "def update_agent_position(state):\n",
        "    \"\"\"Move the agent to the given state.\"\"\"\n",
        "    row, col = state_to_coords(state)\n",
        "    x1, y1 = col * cell_size + 5, row * cell_size + 5\n",
        "    x2, y2 = x1 + cell_size - 10, y1 + cell_size - 10\n",
        "    canvas.coords(agent, x1, y1, x2, y2)\n",
        "    root.update()\n",
        "    time.sleep(0.05)\n",
        "\n",
        "# -----------------------------\n",
        "# Training phase\n",
        "# -----------------------------\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = 0  # start at top-left corner\n",
        "    update_agent_position(state)\n",
        "    print(\"Episode:\", ep)\n",
        "\n",
        "    while state != terminal_state:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward = step_env(state, action)\n",
        "\n",
        "        # Q-learning update\n",
        "        Q[state, action] = Q[state, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
        "        )\n",
        "\n",
        "        state = next_state\n",
        "        update_agent_position(state)\n",
        "\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(\"Training complete! Learned Q-table:\")\n",
        "print(Q)\n",
        "\n",
        "# -----------------------------\n",
        "# Testing phase: Run Optimal Policy\n",
        "# -----------------------------\n",
        "\n",
        "test_state = 0\n",
        "test_path = []\n",
        "max_test_steps = 50\n",
        "\n",
        "def step_policy():\n",
        "    global test_state, test_path\n",
        "\n",
        "    if test_state == terminal_state:\n",
        "        print(\"Optimal path found:\", test_path)\n",
        "        return\n",
        "\n",
        "    if len(test_path) > max_test_steps:\n",
        "        print(\"Stopping test: exceeded max steps. Path so far:\", test_path)\n",
        "        return\n",
        "\n",
        "    qvals = Q[test_state]\n",
        "    max_q = np.max(qvals)\n",
        "    best_actions = np.where(qvals == max_q)[0]\n",
        "    action = np.random.choice(best_actions)\n",
        "\n",
        "    test_state, _ = step_env(test_state, action)\n",
        "    test_path.append(test_state)\n",
        "    update_agent_position(test_state)\n",
        "\n",
        "    root.after(300, step_policy)\n",
        "\n",
        "def run_optimal_policy():\n",
        "    global test_state, test_path\n",
        "    test_state = 0\n",
        "    test_path = [test_state]\n",
        "    update_agent_position(test_state)\n",
        "    step_policy()\n",
        "\n",
        "# Button\n",
        "button = tk.Button(root, text=\"Run Optimal Policy\", command=run_optimal_policy)\n",
        "button.pack()\n",
        "\n",
        "root.mainloop()\n"
      ]
    }
  ]
}